{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f52c9-7b59-4270-ba56-c9db580f3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32206b8-f9fc-4f41-8ff6-5fd0650bedae",
   "metadata": {},
   "source": [
    "#### Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary least squares (OLS), but by an estimator, called ridge estimator, that, albeit biased, has lower variance than the OLS estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d891c-dfe8-4db7-9399-ac4655b9531c",
   "metadata": {},
   "source": [
    "#### Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478d08-7ad6-48f7-9155-faad04da652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63182a00-07d6-407f-828e-25a1dfa719a2",
   "metadata": {},
   "source": [
    "#### The assumptions are the same as those used in regular multiple regression: linearity, constant variance (no outliers), and independence. Since ridge regression does not provide confidence limits, normality need not be assumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37efcf-23d1-439a-ad80-06edbfd5e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7048d-ea70-48f2-b52e-945494f64e24",
   "metadata": {},
   "source": [
    "#### Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782a3d0-3f17-4f54-8c12-26c6b016394d",
   "metadata": {},
   "source": [
    "#### When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.\n",
    "-- As λ increases, more and more coefficients are set to zero and eliminated.\n",
    "\n",
    "-- When λ = ∞, all coefficients are eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea470aa-b5bb-4a2f-bb31-6c5af4809f4a",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "#### This is a widely used and traditional method that performs hyperparameter tuning to determine the optimal values for a given model. Grid search works by trying every possible combination of parameters you want to try in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1e7cb-42e1-4183-9d09-22d4801b077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a8fcd-daaf-4c45-ba53-3dfd8b71efc5",
   "metadata": {},
   "source": [
    "#### You could see ridge regression as doing the feature 'selection' in a nuanced way by reducing the size of the coefficients instead of setting them equal to zero. You could elliminate the features with the smaller coefficients*, but it is a bit crude method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7db919-f983-414a-84b3-1f8c97d86dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca26058-82ed-4058-8f2a-2448b2b9f433",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Multicollinearity happens when predictor variables exhibit a correlation among themselves. Ridge regression aims at reducing the standard error by adding some bias in the estimates of the regression. The reduction of the standard error in regression estimates significantly increases the reliability of the estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd72e4-c11c-48d0-b81d-0c057e2278d9",
   "metadata": {},
   "source": [
    "#### When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ba917-a48d-4746-af75-ce2d25b44464",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d92144-ab9d-41b8-8821-480e9e315022",
   "metadata": {},
   "source": [
    "#### Ridge regression is used for regression purpose only as it needs the dependent variable to be continuous. So for your analysis Ridge regression can't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56aab8d-26c4-4842-855b-4906a0150a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55576bd1-610c-4915-9660-c21efa801b15",
   "metadata": {},
   "source": [
    "#### The ridge coefficients are a reduced factor of the simple linear regression coefficients and thus never attain zero values but very small values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fa833-27c0-4ffd-bf89-7e240f263d4f",
   "metadata": {},
   "source": [
    "#### A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc7746-5667-44f5-9e39-74ccf9608e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a62da-3a78-4e7f-837f-5740f49cc2cb",
   "metadata": {},
   "source": [
    "#### The ridge regression technique can be used to predict time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b8c26-cf8d-4ec4-a007-40d6d8cf9fe7",
   "metadata": {},
   "source": [
    "#### Will come to know details in time series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
